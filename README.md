#  Stabilizing RNNs with Gramâ€“Schmidt (QR) Projection

 **Goal**  
Prevent exploding/vanishing gradients in RNNs by projecting the hidden-to-hidden matrix `W_hh` onto the orthogonal manifold during training.

 **Method**  
We apply full QR decomposition on `W_hh` every N steps using `torch.linalg.qr()` and replace it with its orthogonal component `Q`.  
This ensures `â€–W_hh xâ€– = â€–xâ€–` and keeps gradient norms bounded over long sequences.

 **Result Summary**

| Metric              | Vanilla RNN         | QR-RNN (ours)       | Why it matters                     |
|---------------------|----------------------|---------------------|------------------------------------|
| Validation Loss     | â‰ˆâ€¯1.0 (flat)         | â†“â€¯toâ€¯0.01           | Shows real learning occurred       |
| Gradient Norm       | spikes to ~10,000    | stays <â€¯30          | Prevents weight chaos              |
| Spectral Norm       | grows to 2.3         | stays â‰ˆâ€¯1.0         | Keeps signal energy stable         |

ðŸ’¡ **Key Takeaways**
- Orthogonal matrices rotate but donâ€™t stretch â†’ signals donâ€™t explode or vanish.
- QR decomposition is more stable than classical Gram-Schmidt.
- Projecting every 5 steps offers 1.8Ã— training speedup with stability.

